# language_identification
This work tackles the task of generating natural language descriptions from knowledge graph triples by proposing a dual encoding model composed of both a graph encoder, and sequential encoder.

Since the structure of information presented in knowledge graphs in the form of triples is different from the flow of natural language text, the writers posit that this dual encoding method will narrow this structural gap between the data encoder and the text decoder for better text generation. The paper relies on the capability of Graph Convolutional networks (GCN) to encode structural information from the knowledge graph and the capability of LSTMs to encode sequential information. As an intermediary planning step, they introduce a GCN-based planner that reorders the sequence of nodes in the graph to ensure a better information alignment between the input and output. They use an LSTM based decoder conditioned on both encoders. The work is evaluated on a data-to-text generation benchmark: WebNLG dataset and uses a variety of automatic evaluation metrics as well as human evaluation, performing better than previous work in almost all experiments.

Evidently, the main idea of the paper focuses on the dual encoding stage, which interestingly creates two independent encodings for the same input graphs presented to the model. The planning stage is also equally important and proves to be essential due to improved performance compared to the single encoder models, and also through the ablation study. In the latter, there is a performance decrease of almost 7 BLUE score which proves the impact of the planning stage on the quality of generation. One possible weakness in the paper concerns the ablation study where human evaluation was not conducted. While such an evaluation requires additional resources, could have been appropriate in this section. This is because the ablation study shows that for the same changes in the model, and for the same input, some metrics are heavily affected while others are not. Human evaluation could explain this further. 
